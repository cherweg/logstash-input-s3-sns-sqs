:plugin: logstash-input-s3-sns-sqs
:type: input
:default_codec: json

///////////////////////////////////////////
START - GENERATED VARIABLES, DO NOT EDIT!
///////////////////////////////////////////
:version: %VERSION%
:release_date: %RELEASE_DATE%
:changelog_url: %CHANGELOG_URL%
:include_path: ../../../../logstash/docs/include
///////////////////////////////////////////
END - GENERATED VARIABLES, DO NOT EDIT!
///////////////////////////////////////////

[id="plugins-{type}s-{plugin}"]

=== S3 input plugin (via SNS/SQS)

include::{include_path}/plugin_header.asciidoc[]

==== Description

.Compatibility Note
[NOTE]
================================================================================
Starting with Elasticsearch 5.3, there's an {ref}/modules-http.html[HTTP setting]
called `http.content_type.required`. If this option is set to `true`, and you
are using Logstash 2.4 through 5.2, you need to update the Elasticsearch input
plugin to version 4.0.2 or higher.

================================================================================

Read from an S3 bucket, based metadata read from an SQS Topic.
This is useful for reading files from an S3 bucket at scale with 
multiple logstash instances.

Example:
[source,ruby]
input {
  s3snssqs {
    region                     => "eu-central-1"
    s3_default_options         => { "endpoint_discovery" => true }
    queue                      => "logingest-sqs-queue"
    queue_owner_aws_account_id => "111111111111"
    type                       => "sqs-logs"
    tags                       => ["pa-alb-nonlive"]
    sqs_skip_delete            => true
    codec                      => json
    s3_options_by_bucket       => [
        { bucket_name => "logs-bucket-222222222222-.*"
          credentials => { role => "arn:aws:iam::222222222222:role/logging-role" }
        },
        { bucket_name => "logs-bucket-333333333333-eu-central-1"
          credentials => { role => "arn:aws:iam::333333333333:role/logging-role" }
          folders => [
          { key => ".*\/waflogs.*"
            codec => "json_stream"
            type => "waflogs"},
          { key => ".*\/2020"
            codec => "json_lines"
            type => "reports"}]
        }
    ]
  }
}
==== S3-SNS-SQS Input Configuration Options

This plugin supports the following configuration options plus the <<plugins-{type}s-{plugin}-common-options>> described later.

[cols="<,<,<",options="header",]
|=======================================================================
|Setting |Input type|Required
| <<plugins-{type}s-{plugin}-queue>> |a valid queue name|Yes
| <<plugins-{type}s-{plugin}-queue_owner_aws_account_id>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-s3_options_by_bucket>> |<<array,array>>|No
| <<plugins-{type}s-{plugin}-s3_default_options>> |<<hash,hash>>|No
| <<plugins-{type}s-{plugin}-s3_role_session_name>> |<<string,string>>|yes
| <<plugins-{type}s-{plugin}-delete_on_success>>|<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-include_object_properties>>|<<array,array>>|No
| <<plugins-{type}s-{plugin}-from_sns>> |<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-sqs_skip_delete>> |<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-sqs_delete_on_failure>> |<<boolean,boolean>>|No
| <<plugins-{type}s-{plugin}-temporary_directory>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-consumer_threads>> |<<number,number>>|No
| <<plugins-{type}s-{plugin}-visibility_timeout>> |<<number,number>>|No
| <<plugins-{type}s-{plugin}-max_processing_time>> |<<number,number>>|No
|=======================================================================

Also see <<plugins-{type}s-{plugin}-common-options>> for a list of options supported by all
input plugins.

&nbsp;

[id="plugins-{type}s-{plugin}-queue"]
===== `queue` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

a valid SQS queue name

[id="plugins-{type}s-{plugin}-queue_owner_aws_account_id"]
===== `queue_owner_aws_account_id` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

Do you need to change the aws account?

[id="plugins-{type}s-{plugin}-s3_options_by_bucket"]
===== `s3_options_by_bucket` 

  * Value type is <<array,array>>
  * There is no default value for this setting.

If you have credentials per s3 bucket you could overwrite the default.
If you have different formats (json, plain) in you subfolders 
you could overwrite you default by folder.

Example:
[source,ruby]
    s3_options_by_bucket       => [
        { bucket_name => "logs-bucket-222222222222-.*"
          credentials => { role => "arn:aws:iam::222222222222:role/logging-role" }
        },
        { bucket_name => "logs-bucket-333333333333-eu-central-1"
          credentials => { role => "arn:aws:iam::333333333333:role/logging-role" }
          folders => [
          { key => ".*/waflogs.*"
            codec => "json_stream"
            type => "waflogs"},
          { key => ".*/2020"
            codec => "json_lines"
            type => "reports"}]
        }
    ]

[id="plugins-{type}s-{plugin}-s3_default_options"]
===== `s3_default_options` 

  * Value type is <<hash,hash>>
  * There is no default value for this setting.

Add default options for the ruby aws s3::Client
ATTENTION: Dont use symbols for the option key. This will break the logstash config parser.

Example:
[source,ruby]
  s3_default_options         => { "endpoint_discovery" => true }
  
  
[id="plugins-{type}s-{plugin}-s3_role_session_name"]
===== `s3_role_session_name` 

  * Value type is <<string,string>>
  * Default value is "logstash"

Set S3::Clients Session name

[id="plugins-{type}s-{plugin}-delete_on_success"]
===== `delete_on_success` 

  * Value type is <<boolean,boolean>>
  * The default value for this setting is FALSE.

delete files from S3 after success

[id="plugins-{type}s-{plugin}-include_object_properties"]
===== `include_object_properties`

  * Value type is <<array,array>>
  * The default value for this setting is [:last_modified, :content_type, :metadata]


include object properties from S3 files and add as metadata to the logstash event.
This properties are accessible inside of the logstash pipeline:

Example:
[source,ruby]
    mutate {
        add_field => {
            "meta-s3-content_type" => "%{[@metadata][s3][content_type]}"
        }

[id="plugins-{type}s-{plugin}-from_sns"]
===== `from_sns` 

  * Value type is <<boolean,boolean>>
  * Default: true

The message format is different, if the message comes from SNS.

[id="plugins-{type}s-{plugin}-sqs_skip_delete"]
===== `sqs_skip_delete` 

  * Value type is <<boolean,boolean>>
  * Default: false

Skip the delete on success of sqs messages e.g. for local debugging.
Messages will be requeued after visibility timeout.

[id="plugins-{type}s-{plugin}-sqs_delete_on_failure"]
===== `sqs_delete_on_failure`

  * Value type is <<boolean,boolean>>
  * The default value for this setting is FALSE.

Skip delete of sqs messages causing a hard error or timeout inside the poller and try again.
Maybe you are lucky...maybe you are not.

[id="plugins-{type}s-{plugin}-temporary_directory"]
===== `temporary_directory` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

Your tmp dir

[id="plugins-{type}s-{plugin}-consumer_threads"]
===== `consumer_threads` 

  * Value type is <<number,number>>
  * Default: 1

How many SQS reader should be started

[id="plugins-{type}s-{plugin}-s3_role_arn"]

[id="plugins-{type}s-{plugin}-visibility_timeout"]
===== `visibility_timeout` 

  * Value type is <<number,number>>
  * Default: 5 Minutes.

The default visibility timeout for an SQS Message. If there is no
"exit 0, successfull" from the codeblock the SQS message will be
reprocessed after reaching max_processing_time.

[id="plugins-{type}s-{plugin}-max_processing_time"]
===== `max_processing_time`

  * Value type is <<number,number>>
  * Default: 8000 seconds

The hard limit processing an SQS/S3 message block

[id="plugins-{type}s-{plugin}-common-options"]
include::{include_path}/{type}.asciidoc[]

:default_codec!:
